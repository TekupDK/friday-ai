# Current ChatPanel Analysis

**Date**: November 5, 2025  
**Analyzed By**: GitHub Copilot  
**Files Examined**: ChatPanel.tsx, routers.ts, llm.ts

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ChatPanel.tsx                        â”‚
â”‚                      (Frontend)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Conversation â”‚      â”‚   Messages Display    â”‚       â”‚
â”‚  â”‚     List     â”‚      â”‚                       â”‚       â”‚
â”‚  â”‚              â”‚      â”‚  - User messages      â”‚       â”‚
â”‚  â”‚  - New Chat  â”‚      â”‚  - Assistant messages â”‚       â”‚
â”‚  â”‚  - Select    â”‚      â”‚  - SafeStreamdown     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            Chat Input Area                      â”‚    â”‚
â”‚  â”‚  - Model selector (Gemini/Claude/GPT-4o)        â”‚    â”‚
â”‚  â”‚  - Text input + Voice + Send                    â”‚    â”‚
â”‚  â”‚  - Context tracking (email state)               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚       SuggestionsBar (Feature-Flagged)          â”‚    â”‚
â”‚  â”‚  - Action suggestions                           â”‚    â”‚
â”‚  â”‚  - Approval workflow                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                     tRPC: chat.sendMessage
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               server/routers.ts                         â”‚
â”‚                   (Backend Router)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  chat.sendMessage mutation:                             â”‚
â”‚    1. Create user message in DB                         â”‚
â”‚    2. Get conversation history                          â”‚
â”‚    3. Generate title (if first message)                 â”‚
â”‚    4. Build AI messages array                           â”‚
â”‚    5. Call routeAI() with context                       â”‚
â”‚    6. Create assistant message in DB                    â”‚
â”‚    7. Return response + pendingAction                   â”‚
â”‚                                                          â”‚
â”‚  Input Schema:                                          â”‚
â”‚    - conversationId: number                             â”‚
â”‚    - content: string                                    â”‚
â”‚    - model: enum (gemini-2.5-flash, claude, gpt-4o)     â”‚
â”‚    - attachments: array (optional)                      â”‚
â”‚    - context: object (Shortwave-style tracking)         â”‚
â”‚        âœ… page, selectedThreads, openThreadId           â”‚
â”‚        âœ… folder, viewMode, selectedLabels              â”‚
â”‚        âœ… searchQuery, openDrafts                       â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                       routeAI()
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               server/_core/llm.ts                       â”‚
â”‚                   (LLM Abstraction)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  invokeLLM(params):                                     â”‚
â”‚    - Normalizes messages                                â”‚
â”‚    - Checks API keys (Gemini first, OpenAI fallback)    â”‚
â”‚    - Formats payload (Gemini vs OpenAI format)          â”‚
â”‚    - Makes HTTP request                                 â”‚
â”‚    - Returns InvokeResult                               â”‚
â”‚                                                          â”‚
â”‚  Features:                                              â”‚
â”‚    âœ… Multi-content support (text, image_url, file_url) â”‚
â”‚    âœ… Tool calling support                              â”‚
â”‚    âœ… Response format (json_schema, json_object)        â”‚
â”‚    âœ… Automatic API selection (Gemini â†’ OpenAI)         â”‚
â”‚    âœ… Error handling                                    â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Answers to Key Questions

### 1. âœ… Where are requests sent?

**Endpoint**: `tRPC: chat.sendMessage`

**Payload Structure**:

```typescript
{
  conversationId: number,
  content: string,
  model: "gemini-2.5-flash" | "claude-3-5-sonnet" | "gpt-4o" | "manus-ai",
  attachments?: Array<{ url, name, type }>,
  context?: {
    page?: string,              // "email-tab"
    selectedThreads?: string[], // Thread IDs
    openThreadId?: string,      // Currently viewing
    folder?: string,            // inbox, sent, archive
    viewMode?: string,          // list, pipeline, dashboard
    selectedLabels?: string[],
    searchQuery?: string,
    openDrafts?: number
  }
}
```

**Response**:

```typescript
{
  userMessage: Message,
  assistantMessage: Message,
  pendingAction?: {
    id: string,
    type: string,
    params: Record<string, unknown>,
    riskLevel: "low" | "medium" | "high"
  }
}
```

---

### 2. âœ… How is context handled?

#### Conversation History

- **Storage**: Database (conversations + messages tables)
- **Retrieval**: `getConversationMessages(conversationId)`
- **Format**: Array of `{ role, content, createdAt }`
- **Mapping**: Converted to AI messages: `{ role: "user" | "assistant" | "system", content: string }`

#### Email Context (Shortwave-Style)

**Frontend Collection** (ChatPanel.tsx, line 195-217):

```typescript
const rawContext = {
  page: window.location.pathname.includes("/inbox") ? "email-tab" : undefined,
  selectedThreads: Array.from(emailContext.state.selectedThreads),
  openThreadId: emailContext.state.openThreadId || undefined,
  folder: emailContext.state.folder,
  viewMode: emailContext.state.viewMode,
  selectedLabels: emailContext.state.selectedLabels,
  searchQuery: emailContext.state.searchQuery || undefined,
  openDrafts: emailContext.state.openDrafts || undefined,
};

// Clean: remove undefined/null values and empty arrays
const context = Object.fromEntries(
  Object.entries(rawContext).filter(([_, value]) => {
    if (value === undefined || value === null) return false;
    if (Array.isArray(value) && value.length === 0) return false;
    return true;
  })
);
```

**Backend Usage** (routers.ts, line 191-209):

- Context logged for debugging
- Passed to `routeAI()` function
- Used to enhance AI understanding of user's current state

**Key Insight**: Context is ALREADY being tracked and sent! We can leverage this for email-specific AI.

---

### 3. âœ… Which components are reusable?

#### âœ… Highly Reusable (Little to No Changes)

1. **SafeStreamdown** (line 19, line 467)
   - Purpose: Renders markdown safely with streaming support
   - Usage: `<SafeStreamdown content={message.content} />`
   - **Can use as-is** for AI responses in email sidebar

2. **SuggestionsBar** (line 20, line 503-510)
   - Purpose: Shows action suggestions with approval workflow
   - Already implemented and feature-flagged
   - **Can reuse** for email-specific suggestions (e.g., "Draft Reply", "Summarize")

3. **ActionApprovalModal** (line 21, line 542-548)
   - Purpose: User approval for AI actions
   - Features: Risk level display, always approve checkbox
   - **Can reuse** for email actions (send reply, archive, etc.)

4. **LLM Core** (server/\_core/llm.ts)
   - Functions: `invokeLLM()`, message normalization, API routing
   - **Can reuse directly** for email AI requests
   - Already supports Gemini + OpenAI fallback

#### ğŸ”„ Partially Reusable (Minor Modifications)

5. **Chat Input Pattern** (line 511-541)
   - Structure: Model selector + Input + Voice + Send
   - **Needs adaptation**: Simpler UI for email sidebar (no model selector, no voice?)
   - Can reuse: Input component, send button logic

6. **Message Display Pattern** (line 440-498)
   - Structure: User/Assistant message bubbles with animation
   - **Needs adaptation**: Sidebar styling (narrower width, different colors)
   - Can reuse: Animation, role-based styling logic

#### âŒ Not Reusable (Different Use Case)

7. **Conversation List Sidebar** (line 361-430)
   - Too specific to multi-conversation management
   - Email sidebar won't need conversation switching

8. **Auto-Title Generation** (line 226-239)
   - Specific to chat conversations
   - Email threads already have subjects

---

### 4. âœ… How is streaming handled?

**Current Implementation**: NO STREAMING

- Messages are returned as complete strings
- Loading state: `sendMessage.isPending` (line 499-511)
- Display: Animated dots while waiting

**Evidence**:

- `invokeLLM()` returns full `InvokeResult` (not streamed chunks)
- Frontend shows loading animation, then full response
- No `ReadableStream` or `EventSource` usage

**Implication for Email AI**:

- âœ… Simple: Can show spinner, then full summary
- âŒ Slow perceived speed for long summaries
- ğŸ’¡ Future enhancement: Could add streaming later if needed

---

### 5. âœ… Error/Loading States?

#### Loading States (ChatPanel.tsx)

**Mutation Pending** (line 499-511):

```tsx
{
  sendMessage.isPending && (
    <div className="flex justify-start animate-in fade-in">
      <div className="bg-muted border rounded-2xl px-4 py-3">
        <div className="flex gap-1.5">
          <div className="w-2 h-2 bg-primary/60 rounded-full animate-bounce"></div>
          <div
            className="w-2 h-2 animate-bounce"
            style={{ animationDelay: "0.1s" }}
          ></div>
          <div
            className="w-2 h-2 animate-bounce"
            style={{ animationDelay: "0.2s" }}
          ></div>
        </div>
      </div>
    </div>
  );
}
```

**Button Disabled** (line 531-540):

```tsx
<Button
  onClick={handleSendMessage}
  disabled={!inputMessage.trim() || sendMessage.isPending}
  size="icon"
>
```

#### Error Handling

**Frontend** (line 170-174):

```typescript
onError: error => {
  const errorMessage = error.message || "Unknown error occurred";
  toast.error("Failed to send message: " + errorMessage);
  console.error("[ChatPanel] Send message error:", error);
};
```

**Backend** (routers.ts):

```typescript
// Extensive console.log for debugging
console.log("[Chat] sendMessage called:", { ... });
console.log("[Chat] Creating user message...");
console.log("[Chat] User message created, ID:", userMessage.id);
```

**LLM Layer** (llm.ts, line 336-343):

```typescript
if (!response.ok) {
  const errorText = await response.text();
  throw new Error(
    `LLM invoke failed: ${response.status} ${response.statusText} â€“ ${errorText}`
  );
}
```

**Strengths**:

- âœ… Clear error messages to user via toast
- âœ… Console logging for debugging
- âœ… Disabled states prevent duplicate requests

**Weaknesses**:

- âš ï¸ No retry mechanism
- âš ï¸ No timeout handling (could hang indefinitely)
- âš ï¸ No rate limit handling

---

## ğŸ”§ Reusable Pieces for Email AI

### Components We Can Reuse Directly

1. **SafeStreamdown** â†’ Display AI summaries/responses
2. **SuggestionsBar** â†’ Show quick actions (Summarize, Draft Reply, etc.)
3. **ActionApprovalModal** â†’ Approve sending replies
4. **invokeLLM()** â†’ Make AI requests (already has Gemini + OpenAI)

### Patterns We Can Adapt

1. **Context Tracking** â†’ Already collecting email state, just need to enhance
2. **Loading Animation** â†’ Use the bouncing dots for email sidebar
3. **Error Toast** â†’ Reuse same error handling pattern
4. **Message Display** â†’ Adapt styling for narrower sidebar

### What We Need to Build

1. **AIChatSidebar Component** â†’ New layout (collapsible right panel)
2. **Email Context Extraction** â†’ Convert thread â†’ AI-friendly format
3. **Email-Specific Endpoints** â†’ `summarizeEmail`, `draftReply`, etc.
4. **Integration with EmailThreadView** â†’ Add button + sidebar

---

## âš ï¸ Critical Risks Identified

### Risk 1: Context Completeness

**Problem**: Current context only tracks thread IDs, not actual email content

**Current State**:

```typescript
context: {
  openThreadId: "thread-abc123", // âŒ Just ID
  // Missing: subject, from, to, body, timestamp
}
```

**What We Need**:

```typescript
emailContext: {
  threadId: "thread-abc123",
  subject: "Re: Renovation quote",
  from: { name: "John Doe", email: "john@example.com" },
  to: [{ name: "Me", email: "me@tekup.dk" }],
  messages: [
    { from: "john@...", body: "...", timestamp: "..." },
    { from: "me@...", body: "...", timestamp: "..." }
  ]
}
```

**Mitigation**: Build email context extraction in spike prototype (Task 0.2)

---

### Risk 2: No Streaming

**Problem**: Long summaries will feel slow (waiting for full response)

**Current**: User waits â†’ Full response appears  
**Better**: User sees partial response streaming in

**Impact**: Medium (UX issue, not blocker)

**Mitigation**:

- Phase 0-1: Accept non-streaming (simpler implementation)
- Phase 3: Add streaming if users complain about speed

---

### Risk 3: Rate Limits Not Handled

**Problem**: No queue, retry, or rate limit detection

**Evidence**:

```typescript
// llm.ts - No rate limit handling
if (!response.ok) {
  throw new Error(`LLM invoke failed: ${response.status}`);
  // âŒ Doesn't check if 429 (rate limit)
}
```

**Mitigation**:

- Phase 0-1: Accept risk (low traffic during testing)
- Phase 3: Add proper error handling (Task 12)

---

## ğŸ’¡ Key Learnings

### 1. Context Infrastructure Already Exists! ğŸ‰

We don't need to build context tracking from scratch. The `useEmailContext()` hook and context passing to AI is already implemented. We just need to:

- âœ… Enhance context with actual email content (not just IDs)
- âœ… Add email-specific fields (subject, from, body)

### 2. AI Router is Model-Agnostic

`routeAI()` already handles model selection:

- Gemini (primary)
- OpenAI (fallback)
- Claude (supported)

We can specify `preferredModel` per request.

### 3. Action Approval Pattern Exists

`pendingAction` + `ActionApprovalModal` is already built and tested. We can use this for:

- Sending AI-generated replies
- Archiving emails
- Updating pipeline status

### 4. Frontend is Modern & Performant

- React Query caching (staleTime: 30s for conversations, 10s for messages)
- Optimistic UI patterns
- Proper loading/error states
- Animation library (animate-in, fade-in, etc.)

---

## ğŸ¯ Recommendations for Spike

### What to Build in Prototype (Task 0.2)

1. **Minimal Sidebar Component** (20 min)
   - Right panel, fixed width (300px)
   - Close button
   - One "Summarize" button
   - Response display area

2. **Email Context Extraction** (15 min)
   - Hardcode test email data
   - Format: `{ threadId, subject, from, body }`
   - Pass to AI endpoint

3. **New tRPC Endpoint** (15 min)

   ```typescript
   summarizeEmail: protectedProcedure
     .input(
       z.object({
         threadId: z.string(),
         subject: z.string(),
         from: z.string(),
         body: z.string(),
       })
     )
     .mutation(async ({ input, ctx }) => {
       const prompt = `Summarize this email in 2-3 bullet points:
       From: ${input.from}
       Subject: ${input.subject}
       
       ${input.body}`;

       const response = await invokeLLM({
         messages: [{ role: "user", content: prompt }],
       });

       return { summary: response.choices[0].message.content };
     });
   ```

4. **Wire Up** (10 min)
   - Add button to EmailThreadView
   - Open sidebar on click
   - Call tRPC mutation
   - Display response

### What NOT to Build Yet

- âŒ Beautiful UI (use basic shadcn components)
- âŒ Animations (keep it simple)
- âŒ Multiple AI features (just Summarize)
- âŒ Persistent history (in-memory is fine)
- âŒ Error handling polish (just console.log)
- âŒ Streaming (wait for full response)

---

## âœ… Spike Readiness Checklist

- [x] All 5 questions answered
- [x] Identified 4 reusable components (SafeStreamdown, SuggestionsBar, ActionApprovalModal, invokeLLM)
- [x] Identified 3 critical risks (context completeness, no streaming, no rate limits)
- [x] Document created and detailed
- [x] Clear recommendation for prototype scope

---

## ğŸ“‹ Next Steps

1. âœ… **Task 0.1 COMPLETE** - This analysis document
2. â­ï¸ **Task 0.2 START** - Build quick prototype (1 hour MAX)
3. â­ï¸ **Task 0.3** - Test and make GO/NO-GO decision

**Confidence Level**: HIGH - Architecture supports email AI integration well

---

**Analysis completed**: November 5, 2025  
**Time spent**: ~30 minutes  
**Signed off by**: GitHub Copilot
