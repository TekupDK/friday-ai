# LiteLLM Integration Architecture

**Version:** 1.0.0  
**Status:** Design Phase  
**Author:** Friday AI Team  
**Date:** November 9, 2025

---

## ğŸ¯ Overview

LiteLLM serves as our unified AI gateway, providing:

- Single interface to multiple LLM providers
- Automatic failover and retry logic
- Cost tracking and monitoring
- Load balancing across providers
- Rate limiting and quota management

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Friday AI Application                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ HTTP/REST
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LiteLLM Proxy Gateway (Port 4000)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Request Router & Load Balancer             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    Fallback Strategy & Circuit Breaker Pattern       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Cost Tracking & Metrics Collection          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚            â”‚            â”‚
               â”‚ ALL FREE OPENROUTER MODELS ($0.00!)
               â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DeepSeek Chat    â”‚ â”‚ GLM-4.5   â”‚ â”‚ Mistral  â”‚ â”‚  Llama    â”‚
â”‚  (Primary)        â”‚ â”‚(Fallback1)â”‚ â”‚(Fallback2)â”‚ â”‚(Fallback3)â”‚
â”‚  $0.00 FREE âœ…    â”‚ â”‚$0.00 FREEâ”‚ â”‚$0.00 FREEâ”‚ â”‚$0.00 FREE â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ File Structure

```
server/integrations/litellm/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ litellm.config.yaml     # LiteLLM proxy config
â”‚   â””â”€â”€ providers.config.ts     # Provider settings
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.litellm.yml
â”œâ”€â”€ client.ts                   # LiteLLM client wrapper
â”œâ”€â”€ types.ts                    # TypeScript types
â”œâ”€â”€ errors.ts                   # Custom error classes
â”œâ”€â”€ constants.ts                # Constants & defaults
â”œâ”€â”€ fallback/
â”‚   â”œâ”€â”€ strategy.ts             # Fallback cascade logic
â”‚   â”œâ”€â”€ retry.ts                # Retry mechanism
â”‚   â””â”€â”€ circuit-breaker.ts      # Circuit breaker pattern
â”œâ”€â”€ adapters/
â”‚   â””â”€â”€ openrouter-adapter.ts   # OpenRouter response normalization
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ metrics.ts              # Metrics collection
â”‚   â”œâ”€â”€ logger.ts               # Structured logging
â”‚   â””â”€â”€ health.ts               # Health checks
â””â”€â”€ index.ts                    # Main exports

docs/integrations/litellm/
â”œâ”€â”€ ARCHITECTURE.md             # This file
â”œâ”€â”€ DECISIONS.md                # Technical decisions
â”œâ”€â”€ MIGRATION_PLAN.md           # Migration strategy
â”œâ”€â”€ SETUP.md                    # Setup guide
â”œâ”€â”€ API.md                      # API reference
â”œâ”€â”€ MONITORING.md               # Monitoring guide
â””â”€â”€ TROUBLESHOOTING.md          # Common issues
```

---

## ğŸ”„ Request Flow

### Happy Path (Primary Provider Success)

```
1. Friday AI â†’ LiteLLM Client
   â”œâ”€ model: "gpt-4o"
   â”œâ”€ messages: [...]
   â””â”€ fallback: ["claude-3-opus", "gpt-3.5-turbo"]

2. LiteLLM Client â†’ LiteLLM Proxy (localhost:4000)
   â””â”€ POST /chat/completions

3. LiteLLM Proxy â†’ OpenRouter (Primary)
   â””â”€ Uses glm-4.5-air:free model

4. OpenRouter â†’ LiteLLM Proxy
   â””â”€ Success response

5. LiteLLM Proxy â†’ LiteLLM Client
   â””â”€ Normalized response

6. LiteLLM Client â†’ Friday AI
   â””â”€ Success âœ…
```

### Fallback Path (Primary Fails)

```
1. Friday AI â†’ LiteLLM Client
   â””â”€ Same request

2. LiteLLM Proxy â†’ OpenRouter DeepSeek (Primary)
   â””â”€ âŒ Timeout / Rate Limit / Error

3. Circuit Breaker Activates
   â””â”€ Mark DeepSeek as degraded

4. LiteLLM Proxy â†’ OpenRouter GLM-4.5 (Fallback #1 FREE!)
   â””â”€ Use GLM-4.5-air:free model

5. OpenRouter â†’ LiteLLM Proxy
   â””â”€ Success response

6. LiteLLM Client â†’ Friday AI
   â””â”€ Success âœ… (with fallback metadata)
   â””â”€ Still $0.00 cost! ğŸ‰
```

### All Providers Fail

```
1. Try all providers in cascade
2. Log detailed error information
3. Return graceful error to user
4. Metrics record failure
5. Alert if threshold exceeded
```

---

## ğŸ”§ Core Components

### 1. LiteLLM Client (`client.ts`)

**Purpose:** Thin wrapper around LiteLLM proxy  
**Max Lines:** 100  
**Responsibilities:**

- HTTP requests to proxy
- Response normalization
- Basic error handling
- Request timeout management

```typescript
export class LiteLLMClient {
  async chatCompletion(params: ChatParams): Promise<ChatResponse> {
    // Simple HTTP call to proxy
    // Timeout: 30s
    // Retry: Handled by proxy
  }
}
```

### 2. Fallback Strategy (`fallback/strategy.ts`)

**Purpose:** Define provider cascade  
**Max Lines:** 120  
**Responsibilities:**

- Provider priority order
- Fallback decision logic
- Model mapping per provider
- Cost-aware routing

```typescript
export class FallbackStrategy {
  getProviderCascade(model: string): Provider[] {
    // Returns ordered list of providers to try
    // Based on: cost, availability, model support
  }
}
```

### 3. Circuit Breaker (`fallback/circuit-breaker.ts`)

**Purpose:** Prevent cascading failures  
**Max Lines:** 100  
**Responsibilities:**

- Track provider health
- Open/Close circuit based on errors
- Auto-recovery after timeout
- Metrics reporting

**States:**

- **CLOSED:** Normal operation
- **OPEN:** Provider marked as down
- **HALF_OPEN:** Testing recovery

### 4. Retry Logic (`fallback/retry.ts`)

**Purpose:** Smart retry with backoff  
**Max Lines:** 80  
**Responsibilities:**

- Exponential backoff
- Max 3 retries
- Different strategies per error type
- Metrics tracking

### 5. OpenRouter Adapter (`adapters/openrouter-adapter.ts`)

**Purpose:** Normalize OpenRouter responses  
**Max Lines:** 80  
**Responsibilities:**

- Convert OpenRouter format â†’ Standard format
- Handle different FREE model quirks
- Ensure consistent response structure
- Map model-specific parameters

---

## ğŸ“Š Provider Configuration

### Primary: OpenRouter FREE - DeepSeek Chat

```yaml
- model_name: deepseek-chat
  litellm_params:
    model: openrouter/deepseek/deepseek-chat:free
    api_key: env/OPENROUTER_API_KEY
  model_info:
    cost_per_token: 0.0
    max_tokens: 8000
    supports_streaming: true
    quality: high
```

### Fallback 1: OpenRouter FREE - GLM-4.5 Air

```yaml
- model_name: glm-4.5-air
  litellm_params:
    model: openrouter/01-ai/yi-lightning:free
    api_key: env/OPENROUTER_API_KEY
  model_info:
    cost_per_token: 0.0
    max_tokens: 4096
    supports_streaming: true
    quality: medium-high
```

### Fallback 2: OpenRouter FREE - Mistral 7B

```yaml
- model_name: mistral-7b
  litellm_params:
    model: openrouter/mistralai/mistral-7b-instruct:free
    api_key: env/OPENROUTER_API_KEY
  model_info:
    cost_per_token: 0.0
    max_tokens: 8000
    supports_streaming: true
    quality: medium
```

### Fallback 3: OpenRouter FREE - Llama 3.2

```yaml
- model_name: llama-3.2
  litellm_params:
    model: openrouter/meta-llama/llama-3.2-3b-instruct:free
    api_key: env/OPENROUTER_API_KEY
  model_info:
    cost_per_token: 0.0
    max_tokens: 8000
    supports_streaming: true
    quality: medium
```

---

## ğŸ¯ Design Principles

### 1. Single Responsibility

- Each file handles ONE concern
- Max 200 lines per file
- Clear module boundaries

### 2. Fail-Safe Defaults

- Always have fallback option
- Graceful degradation
- Never throw unhandled errors

### 3. Observable System

- Log all requests/responses
- Metrics for every operation
- Health checks at all levels

### 4. Type Safety

- Full TypeScript coverage
- No `any` types
- Comprehensive interfaces

### 5. Testability

- Each module independently testable
- Mock-friendly interfaces
- Clear dependencies

---

## ğŸ” Monitoring Strategy

### Metrics to Track

```typescript
interface Metrics {
  // Request metrics
  totalRequests: number;
  successRate: number;
  averageLatency: number;
  p95Latency: number;
  p99Latency: number;

  // Provider metrics
  providerRequests: Record<Provider, number>;
  providerErrors: Record<Provider, number>;
  fallbackRate: number;

  // Cost metrics
  totalCost: number;
  costPerRequest: number;

  // Error metrics
  errorRate: number;
  errorsByType: Record<ErrorType, number>;
}
```

### Health Checks

- `/health` - Overall system health
- `/health/providers` - Per-provider status
- `/health/circuit-breakers` - Circuit breaker states

### Alerts

- Error rate > 10%
- Fallback rate > 50%
- Any provider down > 5 min
- Response time > 10s

---

## ğŸš€ Deployment Architecture

### Development

```
Friday AI Dev â†’ LiteLLM Local (Docker) â†’ Providers
```

### Staging

```
Friday AI Staging â†’ LiteLLM Staging (K8s) â†’ Providers
```

### Production

```
Friday AI Prod â†’ LiteLLM Prod (K8s + Redis) â†’ Providers
  â”œâ”€ Pod 1 (Active)
  â”œâ”€ Pod 2 (Active)
  â””â”€ Redis (Shared state)
```

---

## ğŸ“ˆ Scalability Considerations

### Current Scale (Phase 1)

- Single Docker container
- 100 req/min expected
- Local state (no Redis needed)
- Simple health checks

### Future Scale (Phase 2+)

- Multiple instances (K8s)
- 1000+ req/min
- Shared state (Redis)
- Advanced monitoring (Prometheus/Grafana)
- Auto-scaling based on load

---

## ğŸ”’ Security Considerations

### API Key Management

- Never hardcode keys
- Use environment variables
- Rotate keys quarterly
- Different keys per environment

### Request Validation

- Validate all inputs (Zod)
- Sanitize user content
- Rate limiting per user
- Request size limits

### Response Handling

- Never expose provider errors directly
- Sanitize error messages
- Log sensitive data separately
- GDPR compliance

---

## âš¡ Performance Targets

### Response Times

- **p50:** < 1s
- **p95:** < 3s
- **p99:** < 5s
- **Timeout:** 30s

### Availability

- **Uptime:** 99.9%
- **Max downtime:** 43 min/month
- **MTTR:** < 5 min

### Throughput

- **Current:** 100 req/min
- **Target:** 1000 req/min
- **Peak:** 2000 req/min

---

## ğŸ”„ Migration Strategy

### Phase 1: Setup (Week 1, Day 1-2)

1. Install LiteLLM
2. Configure providers
3. Test locally
4. Document setup

### Phase 2: Integration (Week 1, Day 3)

1. Create client wrapper
2. Implement fallback
3. Add monitoring
4. Write tests

### Phase 3: Migration (Week 1, Day 4)

1. Update Friday Docs AI calls
2. Test with staging data
3. Monitor metrics
4. Fix issues

### Phase 4: Rollout (Week 1, Day 5)

1. Deploy to staging
2. 24h monitoring
3. Production deployment
4. Gradual rollout (10% â†’ 100%)

---

## âœ… Success Criteria

### Phase 1 Complete When:

- [ ] LiteLLM running locally
- [ ] All providers configured
- [ ] Fallback logic working
- [ ] Health checks passing
- [ ] Metrics being collected
- [ ] Tests passing (>80% coverage)
- [ ] Documentation complete
- [ ] Team approved

### Production Ready When:

- [ ] Staging stable for 24h
- [ ] No critical bugs
- [ ] Performance targets met
- [ ] Monitoring active
- [ ] Alerts configured
- [ ] Runbook complete
- [ ] Team trained

---

## ğŸ“ Next Steps

1. **Review this architecture** with team
2. **Create DECISIONS.md** (technical decisions)
3. **Create MIGRATION_PLAN.md** (detailed migration)
4. **Get approval** to proceed
5. **Start Task 1.2** (Environment Setup)

---

**Status:** âœ… Architecture Designed  
**Next:** Review & Approve  
**Estimated Time:** 30 min review
